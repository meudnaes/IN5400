{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional exercise week 5: Implement a convolutional neural network\n",
    "\n",
    "IN5400 / IN9400 - Machine Learning for Image Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You will later in this course implement convolutional neural networks. If you want to get started already now, then try to update the code you implemented in last week's coding exercise to use a convolutional neural network instead of a dense neural network:\n",
    "\n",
    "- We recommend that you proceed by making a copy of your solution of last week's coding exercise and modify the \"Model\" class to use some convolutional layers and then one or a few fully-connected layers at the end. For simplicity, please also copy the \"utils\" folder and any \"data\" folder that you used in last week's coding exercise. As the 28x28 images are being concatenated in \"datasetFashionMNIST\", the input to the class method \"forward\" in the \"Model\" class will still have the shape [batch size, 28x28], but you can simply reshape them back to images at the beginning of \"forward\", e.g. by using \"out = x.reshape(x.size(0), 1, 28, 28)\" and then process and return \"out\".\n",
    "\n",
    "- You can use \"nn.Conv2d\" to create convolutional layers and \"nn.MaxPool2d\" to create pooling layers (\"torch.nn\" is imported as \"nn\"). Before you apply any fully-connected layers in the class method \"forward\" in the \"Model\" class, you should convert the activation map of each sample to a vector, e.g. by using \"out = out.view(out.size(0), -1)\".\n",
    "\n",
    "- You may find it interesting to re-experiment with different values for the batch size, learning rate and number of epochs trained, all specified in the dictionary \"config\". You might also want to set \"use_cuda\" to \"True\" in \"config\" if you have a GPU available.\n",
    "\n",
    "If you want to implement a convolutional neural network but do not want to modify your solution of last week's coding exercise, then you can instead modify the code below. It is based on the solution file of last week's coding exercise, but the \"Model\" class is modified and now raises NotImplementedError where you shall be coding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with MNIST Fashion and PyTorch\n",
    "\n",
    "---\n",
    "Your task is to use PyTorch to build a model and train a neural network on the MNIST Fashion dataset. Before you can start, you need to have access to the MNIST Fashion dataset. If you use an IFI computer, the default path given in this Jupyter Notebook file will root you to the data. If you work on any other computer, you will need to download the MNIST Fashion dataset. You can download the files from: https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion\n",
    "\n",
    "MINST Fashion files:\n",
    "- t10k-images-idx3-ubyte.gz\n",
    "- t10k-labels-idx1-ubyte.gz\n",
    "- train-images-idx3-ubyte.gz\n",
    "- train-labels-idx1-ubyte.gz\n",
    "\n",
    "Do not download the files with rightclick-save as in GitHub, but e.g. with left clicking and using the download button. If you get shape errors in the Dataset and your images files are only as few KBytes small, then you have not successfully downloaded the files.\n",
    "\n",
    "The MNIST Fashion dataset have 10 classes: ['T-shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'].  \n",
    "\n",
    "The training set consists of 60,000 images and the test set consists of 10,000 images. The images are of size [28,28].\n",
    "\n",
    "\n",
    "**Important!**\n",
    "You will need to add code only at locations where a NotImplementedError is raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\"%matplotlib inline\"</b> is used to plot figures within Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils.utility_functions import datasetFashionMNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Handling of the data\n",
    "\n",
    "The following cell creates two instances of \"datasetFashionMNIST\". The \"datasetFashionMNIST\" is a \"torch.utils.data.Dataset\" written for the MNIST Fashion dataset.\n",
    "\n",
    "If you do not use an IFI computer, edit the \"dataPath\" to the location of the MNIST Fashion dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the MNIST Fashion files\n",
    "#dataPath = 'data/MNIST_fashion/'\n",
    "dataPath = '/projects/in5400/MNIST_fashion/'\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = datasetFashionMNIST(dataPath=dataPath, train=True)\n",
    "val_dataset   = datasetFashionMNIST(dataPath=dataPath, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['T-shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "plt.figure(figsize=(18, 16), dpi=80)\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(np.array(train_dataset.labels) == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        img = (train_dataset.images[idx,:]).astype(np.uint8)\n",
    "        img = np.resize(img, (28, 28))   # reshape to 28x28\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To keep track of important parameters, we use dictionary \"config\". When you are done implementing the NotImplementedError's below, you may find it interesting to experiment with different values for the batch size, learning rate and number of epochs trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          'batch_size': 128,\n",
    "          'use_cuda': False,      #True=use Nvidia GPU | False use CPU\n",
    "          'log_interval': 20,     #How often to display (batch) loss during training\n",
    "          'epochs': 20,           #Number of epochs\n",
    "          'learningRate': 0.001\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the data with the instances of \"datasetFashionMNIST\". However, we will for convenience use PyTorch's \"torch.utils.data.DataLoader\" class as it helps us with batching and shuffling of the data. It also makes it possible to use multiple CPU cores/threads to speed up data preprocessing. Your task is to instantiate two data loaders (one for each of the training and validation dataset objects), using PyTorch's dataloader \"torch.utils.data.DataLoader\". Consider if you will use multiple workers and shuffling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Build the model\n",
    "\n",
    "You are now to define the network architecture. The code provided below defines a fully connected neural network (dense neural network) with two hidden layer of size 128 and 64. However, we encourage you to play with the network configuration.\n",
    "\n",
    "The input has shape [batch size, 28x28]. The 28x28 image size are being concatenated in \"datasetFashionMNIST\". Try to change:\n",
    "- The number of layers\n",
    "- The size of the hidden layers\n",
    "- The activation functions\n",
    "\n",
    "\n",
    "Note that the model inherits from \"torch.nn.Module\", which requires the two class methods \"__init__\" and \"forward\". The former defines the layers used by the model, while the latter defines how the layers are stacked inside the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        raise NotImplementedError('Define the layers of your convolutional neural network.')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.reshape(x.size(0), 1, 28, 28)\n",
    "        raise NotImplementedError('Apply the layers of your convolutional neural network to \\'out\\'.')\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an instance of Model\n",
    "model = Model()\n",
    "if config['use_cuda'] == True:\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Define optimizer and loss function\n",
    "\n",
    "Instantiate an optimizer, e.g. stochastic gradient descent, from the \"torch.optim\" module (https://pytorch.org/docs/stable/optim.html) with your model. Remember that we have defined \"learning rate\" inside the config-dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of \"torch.optim.SGD\"\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = config['learningRate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here we want to define the loss function (often called criterion). As we are dealing with a classification problem, the softmax cross entropy loss is an appropriate choice.\n",
    "\n",
    "Hint, have a look here: (https://pytorch.org/docs/stable/nn.html#torch-nn-functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(prediction, labels):\n",
    "    \"\"\"Returns softmax cross entropy loss.\"\"\"\n",
    "    loss = F.cross_entropy(input=prediction, target=labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Set up the training process and train the model\n",
    "\n",
    "You now have all the building blocks needed to set up the training process. You will implement the function \"run_epoch\" which shall loop though a dataset and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, epoch, data_loader, optimizer, is_training, config):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model        (obj): The neural network model\n",
    "        epoch        (int): The current epoch\n",
    "        data_loader  (obj): A pytorch data loader \"torch.utils.data.DataLoader\"\n",
    "        optimizer    (obj): A pytorch optimizer \"torch.optim\"\n",
    "        is_training (bool): Whether to use train (update) the model/weights or not. \n",
    "        config      (dict): Configuration parameters\n",
    "\n",
    "    Intermediate:\n",
    "        totalLoss: (float): The accumulated loss from all batches. \n",
    "                            Hint: Should be a numpy scalar and not a pytorch scalar\n",
    "\n",
    "    Returns:\n",
    "        loss_avg         (float): The average loss of the dataset\n",
    "        accuracy         (float): The average accuracy of the dataset\n",
    "        confusion_matrix (float): A 10x10 matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_training==True: \n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss       = 0 \n",
    "    correct          = 0 \n",
    "    confusion_matrix = np.zeros(shape=(10,10))\n",
    "    labels_list      = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "        if config['use_cuda'] == True:\n",
    "            images = data_batch[0].to('cuda') # send data to GPU\n",
    "            labels = data_batch[1].to('cuda') # send data to GPU\n",
    "        else:\n",
    "            images = data_batch[0]\n",
    "            labels = data_batch[1]\n",
    "\n",
    "        if not is_training:\n",
    "            with torch.no_grad():\n",
    "                prediction = model(images)\n",
    "                loss        = loss_fn(prediction, labels)\n",
    "                total_loss += loss.item()    \n",
    "            \n",
    "        elif is_training:\n",
    "            prediction = model(images)\n",
    "            loss        = loss_fn(prediction, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update the number of correct classifications and the confusion matrix\n",
    "        predicted_label  = prediction.max(1, keepdim=True)[1][:,0]\n",
    "        correct          += predicted_label.eq(labels).cpu().sum().numpy()\n",
    "        confusion_matrix += metrics.confusion_matrix(labels.cpu().numpy(), predicted_label.cpu().numpy(), labels=labels_list)\n",
    "\n",
    "        # Print statistics\n",
    "        #batchSize = len(labels)\n",
    "        if batch_idx % config['log_interval'] == 0:\n",
    "            print(f'Epoch={epoch} | {(batch_idx+1)/len(data_loader)*100:.2f}% | loss = {loss:.5f}')\n",
    "\n",
    "    loss_avg         = total_loss / len(data_loader)\n",
    "    accuracy         = correct / len(data_loader.dataset)\n",
    "    confusion_matrix = confusion_matrix / len(data_loader.dataset)\n",
    "\n",
    "    return loss_avg, accuracy, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here is where the action takes place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "train_loss = np.zeros(shape=config['epochs'])\n",
    "train_acc  = np.zeros(shape=config['epochs'])\n",
    "val_loss   = np.zeros(shape=config['epochs'])\n",
    "val_acc    = np.zeros(shape=config['epochs'])\n",
    "train_confusion_matrix = np.zeros(shape=(10,10,config['epochs']))\n",
    "val_confusion_matrix   = np.zeros(shape=(10,10,config['epochs']))\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    train_loss[epoch], train_acc[epoch], train_confusion_matrix[:,:,epoch] = \\\n",
    "                               run_epoch(model, epoch, train_loader, optimizer, is_training=True, config=config)\n",
    "\n",
    "    val_loss[epoch], val_acc[epoch], val_confusion_matrix[:,:,epoch]     = \\\n",
    "                               run_epoch(model, epoch, val_loader, optimizer, is_training=False, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5. Plot the train and validation losses\n",
    "Plot the loss and the accuracy as a function of epochs to monitor the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and the accuracy in training and validation\n",
    "#plt.figure()\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "# plt.subplots_adjust(hspace=2)\n",
    "ax.plot(train_loss, 'b', label='train loss')\n",
    "ax.plot(val_loss, 'r', label='validation loss')\n",
    "ax.grid()\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "ax.legend(loc='upper right', fontsize=16)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "ax.plot(train_acc, 'b', label='train accuracy')\n",
    "ax.plot(val_acc, 'r', label='validation accuracy')\n",
    "ax.grid()\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "val_acc_max = np.max(val_acc)\n",
    "val_acc_max_ind = np.argmax(val_acc)\n",
    "plt.axvline(x=val_acc_max_ind, color='g', linestyle='--', label='Highest validation accuracy')\n",
    "plt.title('Highest validation accuracy = %0.1f %%' % (val_acc_max*100), fontsize=16)\n",
    "ax.legend(loc='lower right', fontsize=16)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let us study the accuracy per class on the validation dataset. We use the result from the epoch with highest validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(val_acc)\n",
    "class_accuracy = val_confusion_matrix[:,:,ind]\n",
    "for ii in range(len(classes)):\n",
    "    acc = val_confusion_matrix[ii,ii,ind] / np.sum(val_confusion_matrix[ii,:,ind])\n",
    "    print(f'Accuracy of {str(classes[ii]).ljust(15)}: {acc*100:.01f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In order to see how the network learns to distinguish the different classes as the training progresses, we can plot the confusion matrices after each second epoch as heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "epoch_step                  = 2    \n",
    "set_colorbar_max_percentage = 10 \n",
    "    \n",
    "# Plot confusion matrices\n",
    "ticks = np.linspace(0,9,10)\n",
    "gridspec_kwargs = dict(top=0.9, bottom=0.1, left=0.0, right=0.9, wspace=0.5, hspace=0.2)\n",
    "for i in range(0, config['epochs'], epoch_step):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 16), gridspec_kw=gridspec_kwargs)\n",
    "    im = ax1.imshow(val_confusion_matrix[:, :, i]*100)\n",
    "    ax1.set_title(f'Validation: Epoch #{i}', fontsize=18)\n",
    "    ax1.set_xticks(ticks=ticks)\n",
    "    ax1.set_yticks(ticks=ticks)\n",
    "    ax1.set_yticklabels(classes)\n",
    "    im.set_clim(0.0, set_colorbar_max_percentage)\n",
    "    ax1.set_xticklabels(classes, rotation=45)\n",
    "    ax1.set_ylabel('Prediction', fontsize=16)\n",
    "    ax1.set_xlabel('Ground truth', fontsize=16)\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax     = divider.append_axes('right', size='5%', pad=0.15)\n",
    "    f.colorbar(im, cax=cax, orientation='vertical')\n",
    "    \n",
    "    im = ax2.imshow(train_confusion_matrix[:, :, i]*100)\n",
    "    ax2.set_title(f'Train: Epoch #{i}', fontsize=18)\n",
    "    ax2.set_xticks(ticks=ticks)\n",
    "    ax2.set_yticks(ticks=ticks)\n",
    "    ax2.set_yticklabels(classes)\n",
    "    im.set_clim(0.0, set_colorbar_max_percentage)\n",
    "    ax2.set_xticklabels(classes, rotation=45)\n",
    "    ax2.set_ylabel('Prediction', fontsize=16)\n",
    "    ax2.set_xlabel('Ground truth', fontsize=16)\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax     = divider.append_axes('right', size='5%', pad=0.15)\n",
    "    f.colorbar(im, cax=cax, orientation='vertical')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
